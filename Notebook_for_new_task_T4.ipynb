{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b35f5b0",
   "metadata": {},
   "source": [
    "# Notebook For TASK 4 with Data G4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ce3e4e",
   "metadata": {},
   "source": [
    "## To run this notebook, Follow these steps:\n",
    "\n",
    "1. Keep 'utils.py' file in the same folder\n",
    "2. Update the data path for new data (G4)\n",
    "3. Keep these files in the same folder 'subset_data', 'g1_test', 'g2_test', 'g3_test'\n",
    "4. Choose the model from downloaded model from hugging face\n",
    "5. Change or update parameters-- According to your need. I have used num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8789cecf",
   "metadata": {},
   "source": [
    "### Import all the libraries and functions (from utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c682e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1122c8d",
   "metadata": {},
   "source": [
    "## Function to load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e530b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(train_data, test_data, tokenizer):\n",
    "    # Mapping from BIO tags to numeric IDs for model training\n",
    "    label_map = {\n",
    "        \"O\": 0,\n",
    "        \"B-treatment\": 1, \"I-treatment\": 2,\n",
    "        \"B-chronic_disease\": 3, \"I-chronic_disease\": 4,\n",
    "        \"B-cancer\": 5, \"I-cancer\": 6,\n",
    "        \"B-allergy_name\": 7, \"I-allergy_name\": 8\n",
    "    }\n",
    "\n",
    "    # Print status\n",
    "    print(\"Preprocessing Function\")\n",
    "    \n",
    "    # Adjust range of tags and parse tags into structured format\n",
    "    train_data['new_tags'] = train_data['tags'].apply(adjust_ranges)\n",
    "    test_data['new_tags'] = test_data['tags'].apply(adjust_ranges)\n",
    "    \n",
    "    train_data['parsed_tags'] = train_data['new_tags'].apply(parse_tags)\n",
    "    test_data['parsed_tags'] = test_data['new_tags'].apply(parse_tags)\n",
    "    \n",
    "    # Convert parsed tags to BIO format\n",
    "    train_data['bio_tags'] = train_data['parsed_tags'].apply(to_bio)\n",
    "    test_data['bio_tags'] = test_data['parsed_tags'].apply(to_bio)\n",
    "    \n",
    "    # Ensure 'text' is of type string\n",
    "    train_data['text'] = train_data['text'].astype(str)\n",
    "    test_data['text'] = test_data['text'].astype(str)\n",
    "    \n",
    "    # Tokenize text and align tokens with labels\n",
    "    train_encodings = tokenize_and_align_labels(train_data['text'].tolist(), tokenizer)\n",
    "    test_encodings = tokenize_and_align_labels(test_data['text'].tolist(), tokenizer)\n",
    "    \n",
    "    # Process labels for NER\n",
    "    train_labels = process_for_ner(train_data, tokenizer, label_map)\n",
    "    test_labels = process_for_ner(test_data, tokenizer, label_map)\n",
    "    \n",
    "    return train_encodings, train_labels, test_encodings, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306c399",
   "metadata": {},
   "source": [
    "## Function to train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c16919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_encodings, train_labels, test_encodings, test_labels, model, num_epochs, learning_rate):\n",
    "    # Create datasets\n",
    "    train_dataset = ClinicalDataset(train_encodings, train_labels)\n",
    "    test_dataset = ClinicalDataset(test_encodings, test_labels)\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Training Started...\")\n",
    "    \n",
    "    # List to keep track of average losses per epoch\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_model(train_loader, model, optimizer, device)\n",
    "        avg_epoch_loss = loss / len(train_loader)\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "        \n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating the model...\")\n",
    "    report = get_classification_report(test_loader, model)\n",
    "\n",
    "    # Save model\n",
    "    print(\"Saving model\")\n",
    "    model.save_pretrained('./model_output')\n",
    "    tokenizer.save_pretrained('./model_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94380d74",
   "metadata": {},
   "source": [
    "## Main function to data loading, training, and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79436f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path, subset_data, g1_test, g2_test, g3_test, model, tokenizer, num_epochs, learning_rate):\n",
    "    print(\"New task data loaded\")\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    train_data, test_data = full_process_and_split_with_stratify(data_path, save_path=None)\n",
    "    print(\"Train test split completed!!\")\n",
    "    \n",
    "    print(\"Combining subset data from G1, G2, and G3 and G4 for model training...\")\n",
    "    train_data_with_subset = pd.concat([train_data, subset_data]).reset_index(drop=True)\n",
    "    test_data_combined = pd.concat([test_data, g1_test, g2_test, g3_test]).reset_index(drop=True)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    train_encodings, train_labels, test_encodings, test_labels = load_and_preprocess_data(\n",
    "        train_data_with_subset, test_data_combined, tokenizer)\n",
    "    \n",
    "    print(\"Encoding Done!!\")\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    train_and_evaluate(train_encodings, train_labels, test_encodings, test_labels, model, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd992ed",
   "metadata": {},
   "source": [
    "## Code to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ba77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths and parameters\n",
    "new_data_path = '../G3.xlsx'  # Path for G4 data, task T4\n",
    "\n",
    "## These datasets are present in the repo\n",
    "subset_data = pd.read_csv('./subset_data_combined_g1_g2_g3.csv')\n",
    "g1_test = pd.read_csv('./test_dataset_for_t1_task_using_G1.csv')\n",
    "g2_test = pd.read_csv('./test_dataset_for_t2_task_using_G2.csv')\n",
    "g3_test = pd.read_csv('./test_dataset_for_t3_task_using_G3.csv')\n",
    "\n",
    "\n",
    "# subset_data = pd.read_csv('../task_dataset/subset_data_combined_g1_g2_g3.csv') ## This data is present in the repo\n",
    "# g1_test = pd.read_csv('../task_dataset/test_dataset_for_t1_task_using_G1.csv') ## This data is present in repo\n",
    "# g2_test = pd.read_csv('../task_dataset/test_dataset_for_t2_task_using_G2.csv') ## This data is present in repo\n",
    "# g3_test = pd.read_csv('../task_dataset/test_dataset_for_t3_task_using_G3.csv') ## This data is present in repo\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35ec74",
   "metadata": {},
   "source": [
    "### You can choose model 'FILENAME' from here:\n",
    "\n",
    "##### Model for Task 1\n",
    "-- FILENAME = \"biobert_complete_model_G1_trained.pth\"\n",
    "\n",
    "##### Model for Task 2\n",
    "-- FILENAME = \"biobert_complete_model_G1+G2_trained.pth\"\n",
    "\n",
    "##### Model for Task 3\n",
    "-- FILENAME = \"biobert_complete_model_G1+G2+G3_trained.pth\"\n",
    "\n",
    "\n",
    "##### Combined model using G1+G2+G3\n",
    "-- FILENAME = \"biobert_combined_trained_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770b3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models from Hugging Face Hub\n",
    "FILENAME = \"biobert_complete_model_G1+G2+G3_trained.pth\"\n",
    "\n",
    "\n",
    "model = torch.load(hf_hub_download(repo_id=\"madhu147/miimansa_ner_project\", filename=FILENAME), encoding='latin1')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dmis-lab/biobert-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8812710",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New task data loaded\n",
      "Train test split completed!!\n",
      "Combining subset data from G1, G2, and G3 for model training...\n",
      "Preprocessing Function\n",
      "Encoding Done!!\n",
      "Training Started...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████| 332/332 [10:57<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.043861489267690186\n",
      "Evaluating the model...\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "         cancer      0.884     0.911     0.898      7343\n",
      "              O      0.952     0.920     0.936     50255\n",
      "   allergy_name      0.783     0.829     0.805      1413\n",
      "chronic_disease      0.887     0.925     0.905     19204\n",
      "      treatment      0.902     0.926     0.914     20554\n",
      "\n",
      "       accuracy                          0.920     98769\n",
      "      macro avg      0.882     0.902     0.891     98769\n",
      "   weighted avg      0.921     0.920     0.920     98769\n",
      "\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "# Run main function\n",
    "main(new_data_path, subset_data, g1_test, g2_test, g3_test, model, tokenizer, num_epochs, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
