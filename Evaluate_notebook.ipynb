{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec526158",
   "metadata": {},
   "source": [
    "# Notebook to Re-Produce the Results for each Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9ab9f",
   "metadata": {},
   "source": [
    "## This notebook takes Model, Task and Datas as main input. \n",
    "\n",
    "Kindly provide nacessary datas according to each task. \n",
    "\n",
    "1. For Task1-- G1_data path is enough\n",
    "2. For Task2-- G1_data and G2_data both are required!!\n",
    "3. For Task3-- G1_data, G2_data, and G3_data all are required!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad8672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import*\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc7ae6",
   "metadata": {},
   "source": [
    "## Function to load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df8f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(test_data, tokenizer):\n",
    "    # Define label mappings\n",
    "    label_map = {\n",
    "        \"O\": 0,\n",
    "        \"B-treatment\": 1, \"I-treatment\": 2,\n",
    "        \"B-chronic_disease\": 3, \"I-chronic_disease\": 4,\n",
    "        \"B-cancer\": 5, \"I-cancer\": 6,\n",
    "        \"B-allergy_name\": 7, \"I-allergy_name\": 8\n",
    "    }\n",
    "    \n",
    "    # Adjust ranges and parse tags for NER\n",
    "    test_data['new_tags'] = test_data['tags'].apply(adjust_ranges)\n",
    "    test_data['parsed_tags'] = test_data['new_tags'].apply(parse_tags)\n",
    "    test_data['bio_tags'] = test_data['parsed_tags'].apply(to_bio)\n",
    "    test_data['text'] = test_data['text'].astype(str)\n",
    "    \n",
    "    # Tokenize text and align labels with tokens\n",
    "    test_encodings = tokenize_and_align_labels(test_data['text'].tolist(), tokenizer)\n",
    "    test_labels = process_for_ner(test_data, tokenizer, label_map)\n",
    "    \n",
    "    return test_encodings, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c1927",
   "metadata": {},
   "source": [
    "## Function to train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd73c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(test_encodings, test_labels, model):\n",
    "    # Create test dataset and dataloader\n",
    "    test_dataset = ClinicalDataset(test_encodings, test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Evaluate and print classification report\n",
    "    print(\"Evaluating the model...\")\n",
    "    report = get_classification_report(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c650e2",
   "metadata": {},
   "source": [
    "## Main function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be749e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, tokenizer, task, data_path_g1, data_path_g2=None, data_path_g3=None):\n",
    "    # Conditional flow based on the task\n",
    "    if task == 1:\n",
    "        print(\"Task:\", task)\n",
    "        train_data, test_data = full_process_and_split_with_stratify(data_path_g1, save_path=None)\n",
    "        print(\"Train test split completed!!\")\n",
    "        test_encodings, test_labels = load_and_preprocess_data(test_data, tokenizer)\n",
    "        print(\"Encoding Done!!\")\n",
    "        train_and_evaluate(test_encodings, test_labels, model)\n",
    "    elif task == 2:\n",
    "        if data_path_g2 is None:\n",
    "            print(\"Please provide data G1, G2 data both to Proceed for Task 2 or Try Task 1\")\n",
    "        else:\n",
    "            print(\"Task:\", task)\n",
    "            train_data_g1, test_data_g1 = full_process_and_split_with_stratify(data_path_g1, save_path=None)\n",
    "            train_data_g2, test_data_g2 = full_process_and_split_with_stratify(data_path_g2, save_path=None)\n",
    "            print(\"Combining G1_test data and G2_test data\")\n",
    "            test_data = pd.concat([test_data_g1, test_data_g2]).reset_index(drop=True)\n",
    "            print(\"Train test split completed!!\")\n",
    "            test_encodings, test_labels = load_and_preprocess_data(test_data, tokenizer)\n",
    "            print(\"Encoding Done!!\")\n",
    "            train_and_evaluate(test_encodings, test_labels, model)\n",
    "    elif task == 3:\n",
    "        if data_path_g2 is None or data_path_g3 is None:\n",
    "            print(\"Please provide G1, G2 and G3 datas to Proceed for Task 3 or Try Task 1\")\n",
    "        else:\n",
    "            print(\"Task:\", task)\n",
    "            train_data_g1, test_data_g1 = full_process_and_split_with_stratify(data_path_g1, save_path=None)\n",
    "            train_data_g2, test_data_g2 = full_process_and_split_with_stratify(data_path_g2, save_path=None)\n",
    "            train_data_g3, test_data_g3 = full_process_and_split_with_stratify(data_path_g3, save_path=None)\n",
    "            print(\"Combining G1_test data, G2_test data and G3_test data\")\n",
    "            test_data = pd.concat([test_data_g1, test_data_g2, test_data_g3]).reset_index(drop=True)\n",
    "            print(\"Train test split completed!!\")\n",
    "            test_encodings, test_labels = load_and_preprocess_data(test_data, tokenizer)\n",
    "            print(\"Encoding Done!!\")\n",
    "            train_and_evaluate(test_encodings, test_labels, model)\n",
    "    else:\n",
    "        print(\"Specify the task number in integers like 1, 2, 3 and TRY AGAIN !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043aa1af",
   "metadata": {},
   "source": [
    "## Code to Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef10f9",
   "metadata": {},
   "source": [
    "### You can choose model 'FILENAME' from here:\n",
    "\n",
    "##### Model for Task 1\n",
    "-- FILENAME = \"biobert_complete_model_G1_trained.pth\"\n",
    "\n",
    "##### Model for Task 2\n",
    "-- FILENAME = \"biobert_complete_model_G1+G2_trained.pth\"\n",
    "\n",
    "##### Model for Task 3\n",
    "-- FILENAME = \"biobert_complete_model_G1+G2+G3_trained.pth\"\n",
    "\n",
    "\n",
    "##### Combined model using G1+G2+G3\n",
    "-- FILENAME = \"biobert_combined_trained_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8d9704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download and load models from Hugging Face hub\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO_ID = \"madhu147/miimansa_ner_project\"\n",
    "\n",
    "# Load models\n",
    "model1 = torch.load(hf_hub_download(repo_id=REPO_ID, filename=\"biobert_complete_model_G1_trained.pth\"), encoding='latin1')\n",
    "model2 = torch.load(hf_hub_download(repo_id=REPO_ID, filename=\"biobert_complete_model_G1+G2_trained.pth\"), encoding='latin1')\n",
    "model3 = torch.load(hf_hub_download(repo_id=REPO_ID, filename=\"biobert_complete_model_G1+G2+G3_trained.pth\"), encoding='latin1')\n",
    "combined_model = torch.load(hf_hub_download(repo_id=REPO_ID, filename=\"biobert_combined_trained_model.pth\"), encoding='latin1')\n",
    "\n",
    "# Set device for model operations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "model = model3  # Specify the model to use\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9337babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update parameters\n",
    "\n",
    "# Update Data Paths\n",
    "data_path_g1 = '../G1.xlsx'\n",
    "data_path_g2 = '../G2.xlsx'\n",
    "data_path_g3 = '../G3.xlsx'\n",
    "\n",
    "# Specify the task to perform\n",
    "task = 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4714e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: 1\n",
      "Train test split completed!!\n",
      "Encoding Done!!\n",
      "Evaluating the model...\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "   allergy_name      0.755     0.763     0.759       283\n",
      "      treatment      0.910     0.832     0.869      5285\n",
      "         cancer      0.909     0.820     0.862      1820\n",
      "chronic_disease      0.903     0.817     0.858      4912\n",
      "              O      0.879     0.948     0.912     14116\n",
      "\n",
      "       accuracy                          0.889     26416\n",
      "      macro avg      0.871     0.836     0.852     26416\n",
      "   weighted avg      0.891     0.889     0.888     26416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Run main function\n",
    "main(model, tokenizer, task, data_path_g1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d167f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
